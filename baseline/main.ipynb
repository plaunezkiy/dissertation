{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/model'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M7B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MistralTokenizer.from_file(\"/models/M7B/tokenizer.model.v3\")  # change to extracted tokenizer file\n",
    "model = Transformer.from_folder(\"/models/M7B\")  # change to extracted model dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbqa = pd.read_json(\"/datasets/FreebaseQA/FreebaseQA-eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\n",
    "\n",
    "\n",
    "def get_response(prompt):\n",
    "    completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n",
    "\n",
    "    tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "    out_tokens, _ = generate([tokens], model, max_tokens=1024, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "    result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreebaseQA\n",
    "### Ask a question and check the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the female presenter of the Channel 4 quiz show '1001 things you should know'?\n",
      "Answer: [{'AnswersMid': 'm.0216y_', 'AnswersName': ['sandi toksvig']}]\n",
      "Model: The female presenter of the Channel 4 quiz show '1001 Things You Should Know' is Anita Rani. She is a British television presenter and journalist, best known for her work on BBC One's 'The One Show' and Channel 4's 'A New Life in the Sun'. The show '1001 Things You Should Know' is a spin-off of the popular Australian show '1001 Things You Should Know Before You Die'.\n"
     ]
    }
   ],
   "source": [
    "# fbqa.Questions[0].get(\"RawQuestions\", None)\n",
    "for i, r in fbqa.iterrows():\n",
    "    q = r.Questions.get(\"RawQuestion\", None)\n",
    "    print(\"Question:\", q)\n",
    "    parse = r.Questions.get(\"Parses\", [None])[0]\n",
    "    if not parse:\n",
    "        print(f\"error in question: {i}\")\n",
    "        continue\n",
    "    answer = parse.get(\"Answers\")\n",
    "    print(\"Answer:\", answer)\n",
    "    response = get_response(q)\n",
    "    print(\"Model:\", response)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is too elaborate, I'm going to prompt the model to give a shorter answer.\n",
    "\n",
    "To know what length to restrict to, I will analyse the lengths of dataset answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [sandi, toksvig]\n",
       "1        [henry, fonda]\n",
       "2      [terry, gilliam]\n",
       "3      [steve, mcqueen]\n",
       "4    [harold, abrahams]\n",
       "Name: Questions, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = fbqa.Questions.apply(lambda t: t[\"Parses\"][0][\"Answers\"][0][\"AnswersName\"][0].split(\" \"))\n",
    "answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    2\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: Questions, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_lengths = answers.apply(len)\n",
    "answer_lengths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3996.000000\n",
       "mean        1.866867\n",
       "std         0.925670\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         2.000000\n",
       "max         9.000000\n",
       "Name: Questions, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873      [the, shoop, shoop, song, (it's, in, his, kiss)]\n",
       "1144     [let, it, snow!, let, it, snow!, let, it, snow!]\n",
       "1342      [always, look, on, the, bright, side, of, life]\n",
       "2706    [night, at, the, museum:, battle, of, the, smi...\n",
       "Name: Questions, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[answer_lengths > 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The appropriate answer length: under a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_LIMITED_PROMPT = \"\"\"\n",
    "You should answer the question below in under a sentence with no other text but the answer.\n",
    "Question:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fbqa_data(question_row):\n",
    "    \"\"\"\n",
    "    Takes in a dataset row and returns Q and A as strings\n",
    "    \"\"\"\n",
    "    question = question_row.Questions.get(\"RawQuestion\", None)\n",
    "    parse = question_row.Questions.get(\"Parses\", [None])[0]\n",
    "    if not parse:\n",
    "        print(f\"error in question: {question}\")\n",
    "        return question, None\n",
    "    answer = parse.get(\"Answers\")\n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3996/3996 [31:54<00:00,  2.09it/s]  \n"
     ]
    }
   ],
   "source": [
    "# fbqa.Questions[0].get(\"RawQuestions\", None)\n",
    "baseline_results = []\n",
    "for i, r in tqdm(list(fbqa.iterrows())):\n",
    "    q, a = get_fbqa_data(r)\n",
    "    # \n",
    "    # print(\"Question:\", q)\n",
    "    # print(\"Answer:\", a)\n",
    "    prompt = LEN_LIMITED_PROMPT.format(question=q)\n",
    "    # print(\"Prompt:\", prompt)\n",
    "    response = get_response(prompt)\n",
    "    # print(\"Model:\", response)\n",
    "    baseline_results.append(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/datasets/FreebaseQA/results/baseline/m7b.csv\", \"w\") as baseline_file:\n",
    "    writer = csv.writer(baseline_file)\n",
    "    writer.writerow([\"Model\"])\n",
    "    for r in baseline_results:\n",
    "        writer.writerow([r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9/423993629.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bline_fbqa_df[\"Correct\"] = bline_fbqa_df.apply(lambda t: t[1] in t[0], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anita rani</td>\n",
       "      <td>sandi toksvig</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mervyn leroy produced the film 12 angry men.</td>\n",
       "      <td>henry fonda</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terry gilliam</td>\n",
       "      <td>terry gilliam</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>steve mcqueen</td>\n",
       "      <td>steve mcqueen</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>harold abrahams</td>\n",
       "      <td>harold abrahams</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model           Actual  Correct\n",
       "0                                    anita rani    sandi toksvig    False\n",
       "1  mervyn leroy produced the film 12 angry men.      henry fonda    False\n",
       "2                                 terry gilliam    terry gilliam     True\n",
       "3                                 steve mcqueen    steve mcqueen     True\n",
       "4                               harold abrahams  harold abrahams     True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bline_fbqa_df = pd.read_csv(\"/datasets/FreebaseQA/results/baseline/m7b.csv\", header=None)\n",
    "bline_fbqa_df.rename(columns={0: \"Model\"}, inplace=True)\n",
    "bline_fbqa_df[\"Actual\"] = answers.apply(\" \".join)\n",
    "bline_fbqa_df[\"Model\"] = bline_fbqa_df[\"Model\"].apply(str.lower)\n",
    "# check correct answer is present in the output\n",
    "bline_fbqa_df[\"Correct\"] = bline_fbqa_df.apply(lambda t: t[1] in t[0], axis=1)\n",
    "bline_fbqa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.736\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Baseline Accuracy: {sum(bline_fbqa_df.Correct)/len(bline_fbqa_df):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's investigate wrong answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anita rani</td>\n",
       "      <td>sandi toksvig</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mervyn leroy produced the film 12 angry men.</td>\n",
       "      <td>henry fonda</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>foinavon won the 1968 grand national.</td>\n",
       "      <td>red alligator</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mexico (men's football)</td>\n",
       "      <td>poland</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>leeds united</td>\n",
       "      <td>leeds united f.c.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Model             Actual  Correct\n",
       "0                                     anita rani      sandi toksvig    False\n",
       "1   mervyn leroy produced the film 12 angry men.        henry fonda    False\n",
       "11         foinavon won the 1968 grand national.      red alligator    False\n",
       "14                       mexico (men's football)             poland    False\n",
       "15                                  leeds united  leeds united f.c.    False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bline_fbqa_df[~bline_fbqa_df.Correct].to_csv(\"/datasets/FreebaseQA/results/baseline/wrong_m7b.csv\")\n",
    "bline_fbqa_df[~bline_fbqa_df.Correct].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upon inspection, I found the following examples:\n",
    "- 15,leeds united,leeds united f.c.,False\n",
    "- 18,seb coe,sebastian coe,False\n",
    "- 183,america (1851),united states,False\n",
    "- 107,manhattan transfer,the manhattan transfer,False\n",
    "- 133,\"\"\"the 39 steps\"\" is the film.\",the thirty-nine steps,False\n",
    "- 135,pickwick papers,the pickwick papers,False\n",
    "- 183,america (1851),united states,False\n",
    "- 200,peter,saint peter,False\n",
    "- 309,memphis,\"memphis, tennessee\",False\n",
    "Etc.\n",
    "\n",
    "Those are partially correct or have a different spelling (numbers, synonyms)\n",
    "A more precise metric could be ngram overlap between the expected and the actual answers.\n",
    "\n",
    "### Rouge (NGram Recall) is a potential candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model          pickwick papers\n",
       "Actual     the pickwick papers\n",
       "Correct                  False\n",
       "Name: 135, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bline_fbqa_df.iloc[135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.6666666666666666, 'p': 1.0, 'f': 0.7999999952000001},\n",
       "  'rouge-2': {'r': 0.5, 'p': 1.0, 'f': 0.6666666622222223},\n",
       "  'rouge-l': {'r': 0.6666666666666666, 'p': 1.0, 'f': 0.7999999952000001}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(bline_fbqa_df.iloc[135].Model, bline_fbqa_df.iloc[135].Actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what does [Grégoire Colin] appear in</td>\n",
       "      <td>[Before the Rain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Joe Thomas] appears in which movies</td>\n",
       "      <td>[The Inbetweeners Movie, The Inbetweeners 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what films did [Michelle Trachtenberg] star in</td>\n",
       "      <td>[Inspector Gadget, Black Christmas, Ice Prince...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what does [Helen Mack] star in</td>\n",
       "      <td>[The Son of Kong, Kiss and Make-Up, Divorce]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what films did [Shahid Kapoor] act in</td>\n",
       "      <td>[Haider, Jab We Met, Chance Pe Dance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Question  \\\n",
       "0            what does [Grégoire Colin] appear in   \n",
       "1            [Joe Thomas] appears in which movies   \n",
       "2  what films did [Michelle Trachtenberg] star in   \n",
       "3                  what does [Helen Mack] star in   \n",
       "4           what films did [Shahid Kapoor] act in   \n",
       "\n",
       "                                             Answers  \n",
       "0                                  [Before the Rain]  \n",
       "1       [The Inbetweeners Movie, The Inbetweeners 2]  \n",
       "2  [Inspector Gadget, Black Christmas, Ice Prince...  \n",
       "3       [The Son of Kong, Kiss and Make-Up, Divorce]  \n",
       "4              [Haider, Jab We Met, Chance Pe Dance]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaqa = pd.read_csv(\"/datasets/MetaQA/1hop/qa_test.txt\", sep=\"\\t\", header=None)\n",
    "metaqa.rename(columns={0: \"Question\", 1: \"Answers\"}, inplace=True)\n",
    "metaqa.Answers = metaqa.apply(lambda t: t.Answers.split(\"|\"), axis=1)\n",
    "metaqa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the answer lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Before the Rain',\n",
       " 'The Inbetweeners Movie',\n",
       " 'The Inbetweeners 2',\n",
       " 'Inspector Gadget',\n",
       " 'Black Christmas']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mqa_answers = sum(metaqa.Answers.tolist(), [])\n",
    "all_mqa_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19346.000000\n",
       "mean         2.072108\n",
       "std          1.084976\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          2.000000\n",
       "max         13.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqa_lengths = pd.Series(all_mqa_answers).apply(str.split)\n",
    "mqa_lengths.apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your Vice Is a Locked Room and Only I Have the Key',\n",
       " 'Your Vice Is a Locked Room and Only I Have the Key',\n",
       " 'Went to Coney Island on a Mission from God... Be Back by Five',\n",
       " 'The Englishman Who Went Up a Hill But Came Down a Mountain']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqa_lengths[mqa_lengths.apply(len) > 10].apply(lambda t: \" \".join(t)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The answers are also under a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_LIMITED_PROMPT = \"\"\"\n",
    "You should answer the question below in under a sentence with no other text but the answer.\n",
    "If there are multiple answers, separate the answers by \"|\".\n",
    "Do not include \"Answer:\". Produce the answer only.\n",
    "Question:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9947/9947 [3:36:14<00:00,  1.30s/it]   \n"
     ]
    }
   ],
   "source": [
    "baseline_results = []\n",
    "\n",
    "for i, r in tqdm(list(metaqa.iterrows())):\n",
    "    q = r.Question\n",
    "    # print(\"Question:\", q)\n",
    "    # print(\"Answer:\", a)\n",
    "    prompt = LEN_LIMITED_PROMPT.format(question=q)\n",
    "    # print(\"Prompt:\", prompt)\n",
    "    response = get_response(prompt)\n",
    "    # print(\"Model:\", response)\n",
    "    baseline_results.append(response)\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        with open(\"/datasets/MetaQA/results/1hop/bline.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Model\"])\n",
    "            for r in baseline_results:\n",
    "                writer.writerow([str(r)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: put it in a py script and run in detached mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbaseline_results\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_results' is not defined"
     ]
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a584723c8b8a4186bc8fc40628bda120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=\"/models\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts(questions):\n",
    "    return [LEN_LIMITED_PROMPT.format(question=question) for question in questions]\n",
    "\n",
    "def tokenize_questions(questions):\n",
    "    \"\"\"Encodes a list of requests into token IDs.\"\"\"\n",
    "    return [tokenizer.encode(q) for q in questions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/9947 [00:12<3:44:55,  1.36s/it]"
     ]
    }
   ],
   "source": [
    "bline_res = []\n",
    "# batch size\n",
    "for i, r in tqdm(list(metaqa.iterrows())):\n",
    "    q = r.Question\n",
    "    # \n",
    "    prompt = LEN_LIMITED_PROMPT.format(question=q)\n",
    "    # decode\n",
    "    decoded_answer = get_response(prompt)\n",
    "    bline_res.append(decoded_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
