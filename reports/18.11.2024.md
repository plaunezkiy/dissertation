# Datasets, inference setup + baseline

1. Download and explore datasets:
    - FBQA - trivia-type QA over FB15k [1](https://github.com/kelvin-jiang/FreebaseQA)
    - CosmosQA - Comprehension with contextual commonsense reasoning [2](https://github.com/wilburOne/cosmosqa)
    - MetaQA - Movie-based Text QA [3](https://github.com/yuyuz/MetaQA?tab=readme-ov-file)

2. CosmosQA comes with several answer options
    FBQA has 1 or more parses that with an inferential chain lead to the Answer Node
    MetaQA might have 1 or more answers

  How do I evaluate the QA system?
  - 4 options, the model has to choose 1
  - Free text answer:
    - BLEU (ngram precision)
    - ROUGE (ngram recall)
    - METEOR (ngram alignment + preprocessing)


References:
@inproceedings{jiang-etal-2019-freebaseqa,
    title = "{F}reebase{QA}: A New Factoid {QA} Data Set Matching Trivia-Style Question-Answer Pairs with {F}reebase",
    author = "Jiang, Kelvin  and
      Wu, Dekun  and
      Jiang, Hui",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1028",
    doi = "10.18653/v1/N19-1028",
    pages = "318--323",
    abstract = "In this paper, we present a new data set, named FreebaseQA, for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove any false positive in these matched triples. Using this method, we are able to efficiently generate over 54K matches from about 28K unique questions with minimal cost. Our analysis shows that this data set is suitable for model training in factoid QA tasks beyond simpler questions since FreebaseQA provides more linguistically sophisticated questions than other existing data sets.",
}

@misc{huang2019cosmosqamachinereading,
      title={Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning}, 
      author={Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1909.00277},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.00277}, 
}

@inproceedings{zhang2017variational,
  title={Variational Reasoning for Question Answering with Knowledge Graph},
  author={Zhang, Yuyu and Dai, Hanjun and Kozareva, Zornitsa and Smola, Alexander J and Song, Le},
  booktitle={AAAI},
  year={2018}
}