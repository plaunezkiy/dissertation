# Datasets, inference setup + baseline

1. Download and explore datasets:
    - FBQA - trivia-type QA over FB15k [1](https://github.com/kelvin-jiang/FreebaseQA)
    - CosmosQA - Comprehension with contextual commonsense reasoning [2](https://github.com/wilburOne/cosmosqa)
    - MetaQA - Movie-based Text QA [3](https://github.com/yuyuz/MetaQA?tab=readme-ov-file)

2. CosmosQA comes with several answer options
    FBQA has 1 or more parses that with an inferential chain lead to the Answer Node
    MetaQA might have 1 or more answers

  How do I evaluate the QA system?
  - 4 options, the model has to choose 1
  - Free text answer:
    - BLEU (ngram precision)
    - ROUGE (ngram recall)
    - METEOR (ngram alignment + preprocessing)


Notes:
Current situation:
  - 19 weeks before the deadline
  - Busy with coursework (gradually more done on Friday)
  - Got a short-term plan (run the experiments and design a change)

Plan:
The plan right now is to get a basic system running:
dataset, evaluation metrics, inference.
Once that's done, can move on to introducing
knowledge to the model.

Datasets:
  - found and downloaded 3 datasets (each is specific and has a corresponding KB)
  - Each has a set of questions and answers
    - FBQA (20k+4k+4k) - FreeBase
    - CosmosQA (35.6k - 28.5k+3.5k+3.5k) - Atomic
    - MetaQA (300k) - WikiMovies

Results:
  - Got 3 datasets, each has a corresponding KB to be used later
  
  - Thought of option selection at first (4 options - 1 actual + 3 random samples)
  but that's not exactly indicative of how well the model performs 
  at domain-specific reasoning - could just be picking a random answer
  + giving it the answer is a hint in itself ()
  
  - BLEU - measures precision of ngrams + penalty for overly short answers
  - ROUGE - measures recall of ngrams (better as we want the relevant bits among other things)
  so long as the correct ngrams are present in the answer, chances are - an educated
  domain expert (valve engineer) will figure it out and put the puzzle together

  HF provides implementations + libraries

Models:
Picked M7B for local inference
ref to table
- have the framework model agnostic (llama, claude, etc.)

Next steps:
  - Finish the framework
  - Run the experiments
  - Pick apart the implementation (see where I can insert K-BERT methodology)
  - Look at GraphRAG by microsoft


Sample question:
what movies did [Wesley Jonathan] star in?	

Answer:
Crossover|Roll Bounce

LLM (gpt4o-mini):
Wesley Jonathan starred in the following notable movies:
Roll Bounce (2005)
The Hot Chick (2002)
Kickin' It Old Skool (2007)
The Year of the Dog (2007)
He is also known for his roles in TV series such as City Guys and Let's Stay Together.


References:
@inproceedings{jiang-etal-2019-freebaseqa,
    title = "{F}reebase{QA}: A New Factoid {QA} Data Set Matching Trivia-Style Question-Answer Pairs with {F}reebase",
    author = "Jiang, Kelvin  and
      Wu, Dekun  and
      Jiang, Hui",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1028",
    doi = "10.18653/v1/N19-1028",
    pages = "318--323",
    abstract = "In this paper, we present a new data set, named FreebaseQA, for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove any false positive in these matched triples. Using this method, we are able to efficiently generate over 54K matches from about 28K unique questions with minimal cost. Our analysis shows that this data set is suitable for model training in factoid QA tasks beyond simpler questions since FreebaseQA provides more linguistically sophisticated questions than other existing data sets.",
}

@misc{huang2019cosmosqamachinereading,
      title={Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning}, 
      author={Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1909.00277},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.00277}, 
}

@inproceedings{zhang2017variational,
  title={Variational Reasoning for Question Answering with Knowledge Graph},
  author={Zhang, Yuyu and Dai, Hanjun and Kozareva, Zornitsa and Smola, Alexander J and Song, Le},
  booktitle={AAAI},
  year={2018}
}