{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# set to \"cuda:1\" for running in parallel on both GPUs\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)\n",
    "torch.set_default_device(device)\n",
    "import pandas as pd\n",
    "from utils.graph import KGraphPreproc\n",
    "from utils.llm.mistral import MistralLLM\n",
    "from utils.prompt import GRAPH_QA_PROMPT\n",
    "from tqdm import tqdm\n",
    "from utils.file import export_results_to_file\n",
    "import os\n",
    "import networkx as nx\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbkb_graph = KGraphPreproc.get_fbkb_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_path(graph, start, target):\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, start, target)\n",
    "        triplets = []\n",
    "        for s,t in zip(path, path[1:]):\n",
    "            head = fbkb_graph.mid2name.get(s, None)\n",
    "            rel = graph[s][t].get(\"relation\", None)\n",
    "            tail = fbkb_graph.mid2name.get(t, None)\n",
    "            if head and rel and tail:\n",
    "                triplets.append(f'{head}-{rel}-{tail}')\n",
    "        return triplets[:250]\n",
    "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question-ID</th>\n",
       "      <th>RawQuestion</th>\n",
       "      <th>ProcessedQuestion</th>\n",
       "      <th>Parses</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FreebaseQA-eval-2</td>\n",
       "      <td>Who directed the films; The Fisher King (1991)...</td>\n",
       "      <td>who directed the films; the fisher king (1991)...</td>\n",
       "      <td>[{'Parse-Id': 'FreebaseQA-eval-2.P0', 'Potenti...</td>\n",
       "      <td>[['/m/07j6w', '/m/07h5d'], ['/m/04z257', '/m/0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Question-ID                                        RawQuestion  \\\n",
       "2  FreebaseQA-eval-2  Who directed the films; The Fisher King (1991)...   \n",
       "\n",
       "                                   ProcessedQuestion  \\\n",
       "2  who directed the films; the fisher king (1991)...   \n",
       "\n",
       "                                              Parses  \\\n",
       "2  [{'Parse-Id': 'FreebaseQA-eval-2.P0', 'Potenti...   \n",
       "\n",
       "                                            entities  \n",
       "2  [['/m/07j6w', '/m/07h5d'], ['/m/04z257', '/m/0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbqa = pd.read_csv(\"/datasets/FreebaseQA/FbQA-eval-1000.csv\", index_col=0)\n",
    "fbqa.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = MistralLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:22<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# init experiment\n",
    "experiment_name = f\"kb-path\"\n",
    "res_path = f\"/datasets/FreebaseQA/results/{experiment_name}.csv\"\n",
    "results = []\n",
    "id_list = []\n",
    "l = 0\n",
    "# load if preinit'ed\n",
    "if os.path.isfile(res_path):\n",
    "    r_df = pd.read_csv(res_path)\n",
    "    l = len(r_df)\n",
    "    results = list(r_df.Model.values)\n",
    "# run through\n",
    "for c, (i, r) in enumerate(tqdm(list(fbqa.iterrows()))):\n",
    "    id_list.append(i)\n",
    "    if c < l:\n",
    "        continue\n",
    "    paths = ast.literal_eval(r.entities)\n",
    "    context = []\n",
    "    for pair in paths:\n",
    "        context.extend(get_triplet_path(fbkb_graph._graph, *pair))\n",
    "    prompt = GRAPH_QA_PROMPT.format(\n",
    "        context=\";\".join(context),\n",
    "        question=r.RawQuestion\n",
    "    )\n",
    "    response = mistral.get_response(prompt)\n",
    "    results.append(response)\n",
    "    # backup every 10 qs\n",
    "    if c % 10 == 0:\n",
    "        export_results_to_file(res_path, results, id_list)\n",
    "export_results_to_file(res_path, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwq = pd.read_csv(\"/datasets/CWQ/cwq-1000.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>compositionality_type</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>topic_ids</th>\n",
       "      <th>answer_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>WebQTest-1311_e920e31a99d6b7dfbeef110668d3103d</td>\n",
       "      <td>comparative</td>\n",
       "      <td>What inspiration of Antoni Gaudi died later th...</td>\n",
       "      <td>[{'aliases': ['W. Morris'], 'answer': 'William...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/0g84t93', '/m...</td>\n",
       "      <td>['/m/08304']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>WebQTest-1382_b9b879060be6df6cb7cd937a7996f9d9</td>\n",
       "      <td>comparative</td>\n",
       "      <td>What country borders Argentina and has an army...</td>\n",
       "      <td>[{'aliases': ['Brazilian ', 'República Federat...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...</td>\n",
       "      <td>['/m/015fr']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>WebQTest-1382_edc19b9010b39a6a7a1e3926399c8522</td>\n",
       "      <td>comparative</td>\n",
       "      <td>What country bordering Argentina has populatio...</td>\n",
       "      <td>[{'aliases': ['Republic of Chile'], 'answer': ...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...</td>\n",
       "      <td>['/m/01p1v', '/m/0165v', '/m/05v10', '/m/015fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>WebQTrn-3252_34f533dd75026e91fc9acd350e6eeffb</td>\n",
       "      <td>comparative</td>\n",
       "      <td>What countries in which the Niger River flows ...</td>\n",
       "      <td>[{'aliases': [], 'answer': 'Benin', 'answer_id...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...</td>\n",
       "      <td>['/m/0164v']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>WebQTrn-2177_dec2523c78124e170c353878876cff1e</td>\n",
       "      <td>comparative</td>\n",
       "      <td>Where did Caroline Kennedy attend university, ...</td>\n",
       "      <td>[{'aliases': ['Harvard University, main campus...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...</td>\n",
       "      <td>['/m/03ksy', '/m/01mpwj', '/m/01n951']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>WebQTest-1382_73e475413d79895e98983bff8b926f21</td>\n",
       "      <td>superlative</td>\n",
       "      <td>Which country bordering Argentina has the lowe...</td>\n",
       "      <td>[{'aliases': ['Brazilian ', 'República Federat...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...</td>\n",
       "      <td>['/m/015fr']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>WebQTrn-2525_6f63302a5c1425c7a4f31bd93c423f2b</td>\n",
       "      <td>superlative</td>\n",
       "      <td>What college, that has the largest number of u...</td>\n",
       "      <td>[{'aliases': ['TU', 'Temple', 'Temple Universi...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...</td>\n",
       "      <td>['/m/01jt2w']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>WebQTrn-909_86f42ab931739ed1c6ba88a8db93fd0d</td>\n",
       "      <td>superlative</td>\n",
       "      <td>What location with the smallest GNIS feature I...</td>\n",
       "      <td>[{'aliases': ['Minneapolis, Minnesota', 'Henne...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/0g84t93', '/m...</td>\n",
       "      <td>['/m/0fpzwf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>WebQTrn-3358_887e6cbf6fd62ad83a3815bd45a6b28d</td>\n",
       "      <td>superlative</td>\n",
       "      <td>Which politician who held office most recently...</td>\n",
       "      <td>[{'aliases': [\"Long 'Un\", 'The Flatboat Man', ...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/0g84t93', '/m...</td>\n",
       "      <td>['/m/0gzh']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>WebQTrn-2238_c9be001aaf5f69c9067ba4b530ca0a93</td>\n",
       "      <td>superlative</td>\n",
       "      <td>What is the oldest team Reggie Bush played for...</td>\n",
       "      <td>[{'aliases': ['The Phins', 'The Fish', 'The Fi...</td>\n",
       "      <td>['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...</td>\n",
       "      <td>['/m/04vn5']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ID compositionality_type  \\\n",
       "894   WebQTest-1311_e920e31a99d6b7dfbeef110668d3103d           comparative   \n",
       "1329  WebQTest-1382_b9b879060be6df6cb7cd937a7996f9d9           comparative   \n",
       "1235  WebQTest-1382_edc19b9010b39a6a7a1e3926399c8522           comparative   \n",
       "1778   WebQTrn-3252_34f533dd75026e91fc9acd350e6eeffb           comparative   \n",
       "1471   WebQTrn-2177_dec2523c78124e170c353878876cff1e           comparative   \n",
       "...                                              ...                   ...   \n",
       "2898  WebQTest-1382_73e475413d79895e98983bff8b926f21           superlative   \n",
       "3254   WebQTrn-2525_6f63302a5c1425c7a4f31bd93c423f2b           superlative   \n",
       "1901    WebQTrn-909_86f42ab931739ed1c6ba88a8db93fd0d           superlative   \n",
       "2507   WebQTrn-3358_887e6cbf6fd62ad83a3815bd45a6b28d           superlative   \n",
       "2261   WebQTrn-2238_c9be001aaf5f69c9067ba4b530ca0a93           superlative   \n",
       "\n",
       "                                               question  \\\n",
       "894   What inspiration of Antoni Gaudi died later th...   \n",
       "1329  What country borders Argentina and has an army...   \n",
       "1235  What country bordering Argentina has populatio...   \n",
       "1778  What countries in which the Niger River flows ...   \n",
       "1471  Where did Caroline Kennedy attend university, ...   \n",
       "...                                                 ...   \n",
       "2898  Which country bordering Argentina has the lowe...   \n",
       "3254  What college, that has the largest number of u...   \n",
       "1901  What location with the smallest GNIS feature I...   \n",
       "2507  Which politician who held office most recently...   \n",
       "2261  What is the oldest team Reggie Bush played for...   \n",
       "\n",
       "                                                answers  \\\n",
       "894   [{'aliases': ['W. Morris'], 'answer': 'William...   \n",
       "1329  [{'aliases': ['Brazilian ', 'República Federat...   \n",
       "1235  [{'aliases': ['Republic of Chile'], 'answer': ...   \n",
       "1778  [{'aliases': [], 'answer': 'Benin', 'answer_id...   \n",
       "1471  [{'aliases': ['Harvard University, main campus...   \n",
       "...                                                 ...   \n",
       "2898  [{'aliases': ['Brazilian ', 'República Federat...   \n",
       "3254  [{'aliases': ['TU', 'Temple', 'Temple Universi...   \n",
       "1901  [{'aliases': ['Minneapolis, Minnesota', 'Henne...   \n",
       "2507  [{'aliases': [\"Long 'Un\", 'The Flatboat Man', ...   \n",
       "2261  [{'aliases': ['The Phins', 'The Fish', 'The Fi...   \n",
       "\n",
       "                                              topic_ids  \\\n",
       "894   ['/m/0g84t93', '/m/0g84t93', '/m/0g84t93', '/m...   \n",
       "1329  ['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...   \n",
       "1235  ['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...   \n",
       "1778  ['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...   \n",
       "1471  ['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...   \n",
       "...                                                 ...   \n",
       "2898  ['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...   \n",
       "3254  ['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...   \n",
       "1901  ['/m/0g84t93', '/m/0g84t93', '/m/0g84t93', '/m...   \n",
       "2507  ['/m/0g84t93', '/m/0g84t93', '/m/0g84t93', '/m...   \n",
       "2261  ['/m/0g84t93', '/m/0g84t93', '/m/02lw5z', '/m/...   \n",
       "\n",
       "                                             answer_ids  \n",
       "894                                        ['/m/08304']  \n",
       "1329                                       ['/m/015fr']  \n",
       "1235  ['/m/01p1v', '/m/0165v', '/m/05v10', '/m/015fr...  \n",
       "1778                                       ['/m/0164v']  \n",
       "1471             ['/m/03ksy', '/m/01mpwj', '/m/01n951']  \n",
       "...                                                 ...  \n",
       "2898                                       ['/m/015fr']  \n",
       "3254                                      ['/m/01jt2w']  \n",
       "1901                                      ['/m/0fpzwf']  \n",
       "2507                                        ['/m/0gzh']  \n",
       "2261                                       ['/m/04vn5']  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:02<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# init experiment\n",
    "experiment_name = f\"kb-path\"\n",
    "res_path = f\"/datasets/CWQ/results/{experiment_name}.csv\"\n",
    "results = []\n",
    "id_list = []\n",
    "l = 0\n",
    "# load if preinit'ed\n",
    "if os.path.isfile(res_path):\n",
    "    r_df = pd.read_csv(res_path)\n",
    "    l = len(r_df)\n",
    "    results = list(r_df.Model.values)\n",
    "# run through\n",
    "for c, (i, r) in enumerate(tqdm(list(cwq.iterrows()))):\n",
    "    id_list.append(i)\n",
    "    if c < l:\n",
    "        continue\n",
    "    topic_ids = set(ast.literal_eval(r[\"topic_ids\"]))\n",
    "    answer_ids = set(ast.literal_eval(r[\"answer_ids\"]))\n",
    "    paths = [\n",
    "        [start, target] for start in topic_ids for target in answer_ids\n",
    "    ]\n",
    "    context = []\n",
    "    for pair in paths:\n",
    "        context.extend(get_triplet_path(fbkb_graph._graph, *pair))\n",
    "    prompt = GRAPH_QA_PROMPT.format(\n",
    "        context=\";\".join(context),\n",
    "        question=r.question\n",
    "    )\n",
    "    response = mistral.get_response(prompt)\n",
    "    results.append(response)\n",
    "    # backup every 10 qs\n",
    "    if c % 10 == 0:\n",
    "        export_results_to_file(res_path, results, id_list)\n",
    "export_results_to_file(res_path, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
