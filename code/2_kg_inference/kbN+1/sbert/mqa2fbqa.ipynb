{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "# set to \"cuda:1\" for running in parallel on both GPUs\n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(device)\n",
    "torch.set_default_device(device)\n",
    "import Stemmer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.graph import KGraphPreproc\n",
    "from utils.graph.chain import GraphChain\n",
    "from utils.llm.mistral import MistralLLM\n",
    "from utils.prompt import GRAPH_QA_PROMPT, ENTITY_PROMPT\n",
    "from utils.file import export_results_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    # global chain\n",
    "    # del mistral\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    r = chain.invoke(prompt)\n",
    "    return r[\"result\"]\n",
    "\n",
    "\n",
    "def save_results(fpath, data_rows):\n",
    "    with open(fpath, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Model\"])\n",
    "        for r in data_rows:\n",
    "            writer.writerow([str(r)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = MistralLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fbqa_data(question_row):\n",
    "    \"\"\"\n",
    "    Takes in a dataset row and returns Q and A as strings\n",
    "    \"\"\"\n",
    "    question = question_row.get(\"RawQuestion\", None)\n",
    "    return question, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### load the graph\n",
    "fbkb_graph = KGraphPreproc.get_fbkb_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = GraphChain.from_llm(\n",
    "    llm=mistral,\n",
    "    graph=fbkb_graph,\n",
    "    qa_prompt=GRAPH_QA_PROMPT,\n",
    "    entity_prompt=ENTITY_PROMPT,\n",
    "    verbose=False,\n",
    ")\n",
    "chain.sbert_cache_path = \"/datasets/FB15k-237/cache/sbert.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question-ID</th>\n",
       "      <th>RawQuestion</th>\n",
       "      <th>ProcessedQuestion</th>\n",
       "      <th>Parses</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FreebaseQA-eval-2</td>\n",
       "      <td>Who directed the films; The Fisher King (1991)...</td>\n",
       "      <td>who directed the films; the fisher king (1991)...</td>\n",
       "      <td>[{'Parse-Id': 'FreebaseQA-eval-2.P0', 'Potenti...</td>\n",
       "      <td>[['/m/07j6w', '/m/07h5d'], ['/m/04z257', '/m/0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Question-ID                                        RawQuestion  \\\n",
       "2  FreebaseQA-eval-2  Who directed the films; The Fisher King (1991)...   \n",
       "\n",
       "                                   ProcessedQuestion  \\\n",
       "2  who directed the films; the fisher king (1991)...   \n",
       "\n",
       "                                              Parses  \\\n",
       "2  [{'Parse-Id': 'FreebaseQA-eval-2.P0', 'Potenti...   \n",
       "\n",
       "                                            entities  \n",
       "2  [['/m/07j6w', '/m/07h5d'], ['/m/04z257', '/m/0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load q's\n",
    "fbqa = pd.read_csv(\"/datasets/FreebaseQA/FbQA-eval-1000.csv\", index_col=0)\n",
    "fbqa.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2080507.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/model/utils/graph/chain.py:111: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  entity_string = self.entity_extraction_chain.run(question)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3e3bce98764ae69c768ffbbe1c5535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8c1730088740f59f2f2340f7c5a711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec5bee9c13d4725a53fd63a07a67900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f576bf77ec664c048cc66adc3a83bec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac298daefa640429bf2abcd4270765a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a5c417ae4343b2bef8366132abd4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aa8ee6203740c6bd04cb8166643f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfcad6c97254ed6b59c5f4c0bbe8313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4396f001da4d4eb869fa2935f99bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9428bdffd0645e78103ceca2feb1c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00ccb75efea4260a2165c61c2fd0f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking embedding cache\n",
      "Loading embedding cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "329136it [11:51, 462.61it/s]\n",
      "/model/utils/graph/chain.py:148: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.qa_chain(\n",
      "100%|██████████| 1000/1000 [4:00:54<00:00, 14.45s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 807/1000 [13:38:41<2:33:29, 47.72s/it]"
     ]
    }
   ],
   "source": [
    "for depth in [1,2,3]:\n",
    "    print(f\"depth: {depth}\")\n",
    "    # set the depth\n",
    "    chain.exploration_depth = depth\n",
    "    # init experiment\n",
    "    experiment_name = f\"sbert-kb{depth}\"\n",
    "    res_path = f\"/datasets/FreebaseQA/results/{experiment_name}.csv\"\n",
    "    results = []\n",
    "    id_list = []\n",
    "    l = 0\n",
    "    # load if preinit'ed\n",
    "    if os.path.isfile(res_path):\n",
    "        r_df = pd.read_csv(res_path)\n",
    "        l = len(r_df)\n",
    "        results = list(r_df.Model.values)\n",
    "    # run through\n",
    "    for c, (i, r) in enumerate(tqdm(list(fbqa.iterrows()))):\n",
    "        id_list.append(i)\n",
    "        if c < l:\n",
    "            continue\n",
    "        q, a = get_fbqa_data(r)\n",
    "        response = get_response(q)\n",
    "        results.append(response)\n",
    "        # backup every 10 qs\n",
    "        if c % 10 == 0:\n",
    "            export_results_to_file(res_path, results, id_list)\n",
    "    export_results_to_file(res_path, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaqa_graph = KGraphPreproc.get_metaqa_graph()\n",
    "\n",
    "chain = GraphChain.from_llm(\n",
    "    llm=mistral,\n",
    "    graph=metaqa_graph,\n",
    "    qa_prompt=GRAPH_QA_PROMPT,\n",
    "    entity_prompt=ENTITY_PROMPT,\n",
    "    verbose=False,\n",
    ")\n",
    "chain.sbert_cache_path = \"/datasets/MetaQA/cache/sbert.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1hop\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(hop)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load q's\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m metaqa \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets/MetaQA/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test_1000.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m metaqa\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswers\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "for hop in [\"1hop\", \"2hop\", \"3hop\"]:\n",
    "    print(hop)\n",
    "    # load q's\n",
    "    metaqa = pd.read_csv(f\"/datasets/MetaQA/{hop}/test_1000.txt\", header=None, index_col=0)\n",
    "    metaqa.rename(columns={1: \"Question\", 2: \"Answers\"}, inplace=True)\n",
    "\n",
    "\n",
    "    for depth in [2]:\n",
    "        print(f\"depth: {depth}\")\n",
    "        # set the depth\n",
    "        chain.exploration_depth = depth\n",
    "        # init experiment\n",
    "        experiment_name = f\"sbert-kb{depth}\"\n",
    "        res_path = f\"/datasets/MetaQA/results/{hop}/{experiment_name}.csv\"\n",
    "        results = []\n",
    "        id_list = []\n",
    "        l = 0\n",
    "        # load if preinit'ed\n",
    "        if os.path.isfile(res_path):\n",
    "            r_df = pd.read_csv(res_path)\n",
    "            l = len(r_df)\n",
    "            results = list(r_df.Model.values)\n",
    "        # run through\n",
    "        for c, (i, r) in enumerate(tqdm(list(metaqa.iterrows()))):\n",
    "            id_list.append(i)\n",
    "            if c < l:\n",
    "                continue\n",
    "            q = r.Question\n",
    "            response = get_response(q)\n",
    "            results.append(response)\n",
    "            # backup every 10 qs\n",
    "            if c % 10 == 0:\n",
    "                export_results_to_file(res_path, results, id_list)\n",
    "        export_results_to_file(res_path, results, id_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
