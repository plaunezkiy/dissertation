{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "# set to \"cuda:1\" for running in parallel on both GPUs\n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(device)\n",
    "torch.set_default_device(device)\n",
    "import Stemmer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.graph import KGraphPreproc\n",
    "from utils.graph.chain import GraphChain\n",
    "from utils.llm.mistral import MistralLLM\n",
    "from utils.file import export_results_to_file\n",
    "from utils.prompt import GRAPH_QA_PROMPT, ENTITY_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    # global chain\n",
    "    # del mistral\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    r = chain.invoke(prompt)\n",
    "    return r[\"result\"]\n",
    "\n",
    "\n",
    "def save_results(fpath, data_rows):\n",
    "    with open(fpath, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Model\"])\n",
    "        for r in data_rows:\n",
    "            writer.writerow([str(r)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = MistralLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaqa_graph = KGraphPreproc.get_metaqa_graph()\n",
    "\n",
    "chain = GraphChain.from_llm(\n",
    "    llm=mistral,\n",
    "    graph=metaqa_graph,\n",
    "    qa_prompt=GRAPH_QA_PROMPT,\n",
    "    entity_prompt=ENTITY_PROMPT,\n",
    "    verbose=False,\n",
    ")\n",
    "chain.sbert_cache_path = \"/datasets/MetaQA/cache/sbert.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1hop\n",
      "depth: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1698098.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2hop\n",
      "depth: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1813361.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3hop\n",
      "depth: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/model/utils/graph/chain.py:111: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  entity_string = self.entity_extraction_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking embedding cache\n",
      "Loading embedding cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124680it [04:32, 457.96it/s]\n",
      "/model/utils/graph/chain.py:148: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.qa_chain(\n",
      "100%|██████████| 1000/1000 [4:05:31<00:00, 14.73s/it] \n"
     ]
    }
   ],
   "source": [
    "for hop in [\"1hop\", \"2hop\", \"3hop\"]:\n",
    "    print(hop)\n",
    "    # load q's\n",
    "    metaqa = pd.read_csv(f\"/datasets/MetaQA/{hop}/test_1000.txt\", header=None, index_col=0)\n",
    "    metaqa.rename(columns={1: \"Question\", 2: \"Answers\"}, inplace=True)\n",
    "\n",
    "\n",
    "    for depth in [4]:\n",
    "        print(f\"depth: {depth}\")\n",
    "        # set the depth\n",
    "        chain.exploration_depth = depth\n",
    "        # init experiment\n",
    "        experiment_name = f\"sbert-kb{depth}\"\n",
    "        res_path = f\"/datasets/MetaQA/results/{hop}/{experiment_name}.csv\"\n",
    "        results = []\n",
    "        id_list = []\n",
    "        l = 0\n",
    "        # load if preinit'ed\n",
    "        if os.path.isfile(res_path):\n",
    "            r_df = pd.read_csv(res_path)\n",
    "            l = len(r_df)\n",
    "            results = list(r_df.Model.values)\n",
    "        # run through\n",
    "        for c, (i, r) in enumerate(tqdm(list(metaqa.iterrows()))):\n",
    "            id_list.append(i)\n",
    "            if c < l:\n",
    "                continue\n",
    "            q = r.Question\n",
    "            response = get_response(q)\n",
    "            results.append(response)\n",
    "            # backup every 10 qs\n",
    "            if c % 10 == 0:\n",
    "                export_results_to_file(res_path, results, id_list)\n",
    "        export_results_to_file(res_path, results, id_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
