{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.graph import KGraphPreproc\n",
    "from utils.graph.chain import GraphChain\n",
    "from utils.llm.mistral import MistralLLM\n",
    "from utils.prompt import GRAPH_QA_PROMPT, ENTITY_PROMPT\n",
    "from utils.file import export_results_to_file\n",
    "import networkx as nx\n",
    "from utils.evaluation import CWQ_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    global chain\n",
    "    # del mistral\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    r = chain.invoke(prompt)\n",
    "    return r[\"result\"]\n",
    "\n",
    "def get_path_len(row):\n",
    "    for start in row.topic_ids:\n",
    "        for target in row.answer_ids:\n",
    "            try:\n",
    "                if nx.has_path(fbkb_graph._graph, start, target):\n",
    "                    return nx.shortest_path_length(fbkb_graph._graph, start, target)\n",
    "            except nx.NodeNotFound:\n",
    "                continue\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbkb_graph = KGraphPreproc.get_fbkb_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### load the qa, graph, llm\n",
    "cwq_eval = CWQ_Dataset()\n",
    "cwq = cwq_eval.test_set.copy()\n",
    "cwq[\"hops\"] = cwq.apply(get_path_len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = MistralLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = GraphChain.from_llm(\n",
    "    llm=mistral,\n",
    "    graph=fbkb_graph,\n",
    "    qa_prompt=GRAPH_QA_PROMPT,\n",
    "    entity_prompt=ENTITY_PROMPT,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with depth 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/model/utils/graph/chain.py:111: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  entity_string = self.entity_extraction_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking embedding cache\n",
      "Loading embedding cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "329136it [12:00, 456.95it/s]\n",
      "/model/utils/graph/chain.py:148: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.qa_chain(\n",
      "100%|██████████| 1000/1000 [2:07:16<00:00,  7.64s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with depth 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [17:10:49<00:00, 61.85s/it]   \n"
     ]
    }
   ],
   "source": [
    "for depth in [7,6]:\n",
    "    print(f\"Running with depth {depth}\")\n",
    "    # set the depth\n",
    "    chain.exploration_depth = depth\n",
    "    chain.ranking_strategy = \"sbert\"\n",
    "    # init experiment\n",
    "    experiment_name = f\"sbert-kb{depth}\"\n",
    "    res_path = f\"/datasets/CWQ/results/{experiment_name}.csv\"\n",
    "    results = []\n",
    "    id_list = []\n",
    "    l = 0\n",
    "    # load if preinit'ed\n",
    "    if os.path.isfile(res_path):\n",
    "        r_df = pd.read_csv(res_path)\n",
    "        l = len(r_df)\n",
    "        results = list(r_df.Model.values)\n",
    "    # run through\n",
    "    for c, (i, r) in enumerate(tqdm(list(cwq.iterrows()))):\n",
    "        id_list.append(i)\n",
    "        if c < l:\n",
    "            continue\n",
    "        if r[\"hops\"] < depth -1 or r[\"hops\"] > depth + 1:\n",
    "            results.append(\"\")\n",
    "            continue\n",
    "        q = r.question\n",
    "        response = get_response(q)\n",
    "        results.append(response)\n",
    "        # backup every 10 qs\n",
    "        if c % 10 == 0:\n",
    "            export_results_to_file(res_path, results, id_list)\n",
    "    export_results_to_file(res_path, results, id_list)\n",
    "\n",
    "# python -m 2_kg_inference.kbN.bm25.fbqa &\n",
    "# python -m 2_kg_inference.kbN.bm25.mqa &"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
