{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import Stemmer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.graph import KGraphPreproc\n",
    "from utils.llm.mistral import MistralLLM\n",
    "from utils.prompt import GRAPH_QA_PROMPT, ENTITY_PROMPT, NO_CONTEXT_PROMPT, EVALUATE_CONTEXT_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToG Algorithm pseudocode\n",
    "given a list of entities $E^{D-1}$:\n",
    "1. Select all relations $R^D$\n",
    "2. Select top-N relevant $E^{D-1}$-$R^D$-? | ?-$R^D$-$E^{D-1}$\n",
    "3. Extend P by top-N paths\n",
    "4. Decide if enough to answer the question (YES/NO)\n",
    "5. Repeat (NO), return (YES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4, 5}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set([1,2,3])\n",
    "b = set([3,4,5])\n",
    "b.difference(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToGChain(GraphQAChain):\n",
    "    max_exploration_depth: int = 7\n",
    "    beam_width: int = 5\n",
    "    max_triplets: int = 250\n",
    "    plain_qa_chain: LLMChain\n",
    "    evaluate_context_chain: LLMChain\n",
    "\n",
    "    def tog_answer(self, question):\n",
    "        \"\"\"\n",
    "        Think-on-Graph QA implementation\n",
    "        \"\"\"\n",
    "        explored_entities = set()\n",
    "        P = [\n",
    "            # step: [triplet1, ...], \n",
    "        ] # max_depth X beam_width\n",
    "        max_depth = self.max_exploration_depth\n",
    "        beam_width = self.beam_width\n",
    "\n",
    "        # init entities from question\n",
    "        extracted_entities = set(\n",
    "            self.extract_entities(question)\n",
    "        )\n",
    "        unexplored_entities = extracted_entities.difference(explored_entities)\n",
    "        explored_entities.update(unexplored_entities)\n",
    "        # extract all triplets and rank them\n",
    "        triplets = graph.extract_triplets(unexplored_entities)\n",
    "        # ranked_triplets: list[triplet]\n",
    "        ranked_triplets = ranker.rank(question, triplets, top_k=beam_width)\n",
    "        # extend the paths by next step\n",
    "        P.append(ranked_triplets)\n",
    "\n",
    "        # check at least one path to explore is initialised\n",
    "        if len(P):\n",
    "            for depth in range(max_depth):\n",
    "                _run_manager.on_text(f\"Exploring depth: {depth}\", end=\"\\n\", verbose=self.verbose)\n",
    "                ######## Extraction\n",
    "                tail_entities = set(\n",
    "                    map(\n",
    "                        # extract tails\n",
    "                        lambda triplet: triplet[-1],\n",
    "                        P[depth-1]\n",
    "                    )\n",
    "                )\n",
    "                # select unexplored tails and update\n",
    "                unexplored_entities = tail_entities.difference(explored_entities)\n",
    "                explored_entities.update(unexplored_entities)\n",
    "                # extract and rank triplets\n",
    "                triplets = extract_triplets(unexplored_entities)\n",
    "                ranked_triplets = ranker.rank(question, triplets, top_k=beam_width)\n",
    "                # extend the paths by next step\n",
    "                P.append(ranked_triplets)\n",
    "                ######## Reasoning\n",
    "                is_enough = self.check_enough_context_to_answer(question, reasoning_chain=P)\n",
    "                if is_enough:\n",
    "                    return self.generate_answer_with_context(question, reasoning_chain=P)\n",
    "                else:\n",
    "                    continue\n",
    "        # otherwise, use internal knowledge\n",
    "        return self.generate_answer_no_context(question)\n",
    "    \n",
    "    #### GRAPH OPEATIONS\n",
    "    def extract_triplets(self, entities):\n",
    "        subgraph_entities = set()\n",
    "        for entity in entities:\n",
    "            processed_entity = preprocess_text(entity)\n",
    "            node = self.graph.preprocessed_nodes.get(processed_entity)\n",
    "            if node is None:\n",
    "                continue\n",
    "            bfs_nodes = nx.single_source_shortest_path_length(\n",
    "                self.graph._graph,\n",
    "                node,\n",
    "                cutoff=self.exploration_depth\n",
    "            ).keys()\n",
    "            subgraph_entities.update(bfs_nodes)\n",
    "        subgraph = self.graph._graph.subgraph(subgraph_entities)\n",
    "        # \n",
    "        triplets = []\n",
    "        mid2name_dict = self.graph.mid2name\n",
    "        for head, tail, attrs in subgraph.edges(data=True):\n",
    "            rel = attrs.get(\"relation\", None)\n",
    "            triplets.append(\n",
    "                f\"{mid2name_dict.get(head, '')} {rel} {mid2name_dict.get(tail, '')}\"\n",
    "            )\n",
    "\n",
    "    \n",
    "    #### PROMPTING AND CHAINS\n",
    "    def generate_answer_with_context(question, reasoning_chain):\n",
    "        context = \"\\n\".join(reasoning_chain)\n",
    "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)\n",
    "        _run_manager.on_text(context, color=\"green\", end=\"\\n\", verbose=self.verbose)\n",
    "        return self.qa_chain(\n",
    "            {\"question\": question, \"context\": context},\n",
    "            callbacks=_run_manager.get_child(),\n",
    "        )\n",
    "    \n",
    "    def generate_answer_no_context(question):\n",
    "        return self.plain_qa_chain(\n",
    "            {\"question\": question},\n",
    "            callbacks=_run_manager.get_child(),\n",
    "        )\n",
    "    \n",
    "    def check_enough_context_to_answer(question, reasoning_chain):\n",
    "        context = \"\\n\".join(reasoning_chain)\n",
    "        _run_manager.on_text(\"Checking if the context is sufficient:\", end=\"\\n\", verbose=self.verbose)\n",
    "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)\n",
    "        _run_manager.on_text(context, color=\"green\", end=\"\\n\", verbose=self.verbose)\n",
    "        answer = self.evaluate_context_chain(\n",
    "            {\"question\": question, \"context\": context},\n",
    "            callbacks=_run_manager.get_child(),\n",
    "        )\n",
    "\n",
    "        if \"yes\" in answer.strip().lower():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def extract_entities(self, string):\n",
    "        entity_string = self.entity_extraction_chain.run(question)\n",
    "        _run_manager.on_text(\"Entities Extracted:\", end=\"\\n\", verbose=self.verbose)\n",
    "        _run_manager.on_text(\n",
    "            entity_string, color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "        )\n",
    "        entities = get_entities(entity_string)\n",
    "        return entities\n",
    "\n",
    "    #### INITIALIZATION AND INVOKATION\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Extract entities, look up info and answer question.\"\"\"\n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "        question = inputs[self.input_key]        \n",
    "        # \n",
    "        result = self.tog_answer(question)\n",
    "        return {self.output_key: result[self.qa_chain.output_key]}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        qa_prompt: BasePromptTemplate = GRAPH_QA_PROMPT,\n",
    "        qa_no_context_prompt: BasePromptTemplate = NO_CONTEXT_PROMPT,\n",
    "        evaluate_context_prompt: BasePromptTemplate = EVALUATE_CONTEXT_PROMPT,\n",
    "        entity_prompt: BasePromptTemplate = ENTITY_PROMPT,\n",
    "        **kwargs: Any,\n",
    "    ) -> GraphQAChain:\n",
    "        \"\"\"Initialize from LLM.\"\"\"\n",
    "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
    "        plain_qa_chain = LLMChain(llm=llm, prompt=qa_no_context_prompt)\n",
    "        evaluate_context_chain = LLMChain(llm=llm, prompt=evaluate_context_prompt)\n",
    "        entity_chain = LLMChain(llm=llm, prompt=entity_prompt)\n",
    "\n",
    "        return cls(\n",
    "            qa_chain=qa_chain,\n",
    "            plain_qa_chain=plain_qa_chain,\n",
    "            evaluate_context_chain=evaluate_context_chain,\n",
    "            entity_extraction_chain=entity_chain,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbqa = pd.read_json(\"/datasets/FreebaseQA/FreebaseQA-eval.json\")\n",
    "fbkb_graph = KGraphPreproc.get_fbkb_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = MistralLLM()\n",
    "chain = ToGChain.from_llm(\n",
    "    llm=mistral,\n",
    "    graph=fbkb_graph,\n",
    "    verbose=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
