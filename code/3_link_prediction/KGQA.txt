Chapter 5. Discussion 34 5.2 Relevance-Based Methods We employ 2 heuristics that introduce the concept of relevance scoring between the query and triplets on the knowledge graph. Experiments with lexcial BM25 and semantic S-BERT are designed to test the model’s ability to interpret the provided context in 3 scenarios. The scenarios are: under-retrieval ( n−1) where the retrier lacks 1 step from reaching the answer, perfect ( n), and over-retrieval ( n+1) where the context contains more information than it should. These cases show deviations in accuracy based on completeness of the context, which are useful for building autonomous QA agents. When the agent traverses the graph, it is likely to end up in the vicinity of the answer, rather than on the actual node [ 48]. The experiments help understand the degree to which the 3 edge case scenarios affect the final accuracy. BM25 FBQA and MetaQA get a 15-20% increase, whereas CWQ goes below the baseline. For CWQ, neither case yields any improvement, with all results sat just below the baseline, possibly showing that either the context retrieved is incorrect, or the model is incapable of processing the context. As described above (Chapter 3, ComplexWebQues- tions), FBQA contains basic fact retrieval questions, while CWQ needs more complex comprehension, such as comparative or superlative reasoning. MetaQA quite sensitive to context retrieval depth, and exhibits an expected increasing trend from n−1ton+1. One reason this is the case could be the afforementioned Freebase bias and the intricate balance between context knowledge and the model’s internal knowledge. Given low baseline performance and an assumption that little WikiMovies data ended up in Mistral’s training set, we could see a clearer performance distinction between the different scenarios. One way to study this further is to pick a model with a publicly available training data, which does not have Freebase/WikiMovies data in it eliminating the potential bias. BM25 produces an average accuracy of 44% with 3.7% standard deviation, which is an 11% increase from the average baseline level. Low standard deviation shows a relatively stable response to the 3 scenarios, which is desired in an autonomous system. Between the 3 settings, there is little deviation, allowing us to infer 2 main conclusions. First, BM25 hyperparameters1k1andb, might need fine-tuning. This is especially evident on MetaQA, where n+1performs almost 3% better than n, showing that BM25 works better on a large collection of documents. Second, the knowledge graph data retrieved could be only marginally relevant with respect to model’s internal knowledge in QKV matrices (introduced in Chapter 2, self-attention), which take precedence when generating the answer. Both theories require further experiments with more data collected and more detailed analysis. 1Favouring term frequency and document length respectively.Chapter 5. Discussion 35 S-BERT Instead of using exact lexical matches, S-BERT embeds sentences as dense vectors in a latent space that results in similar words appearing together [ 51]. Synonymous words are better identified mathematically, without relying on exact word matches. On average, S-BERT based graph pruning produces a 48% accuracy with a 6% standard deviation. This is higher than BM25, but more sensitive to the depth setting, which is expected. CWQ gets a more notable increase and a clearly improving accuracy with the extra depth of the context provided, almost 11% compared with BM25 and 13% with baseline. FBQA, on the other hand, performs better without additional context by about 1%. MetaQA continues having an increasing trend, from the same n−1accuracy, to a 13% higher mean, which drives the standard deviation to increase to 16%. Overall, S-BERT has the best average accuracy across all datasets at 53% with 6% stan- dard deviation, making it a more sensitive, but better ranking method for constructing the context relevant to answering domain-specific questions. 5.3 ToG-LP Think-on-Graph Link Prediction is a graph traversal algorithm that iteratively constructs several paths along the graph in a beam search manner and lets the main LLM model decide if it needs to continue traversing at the end of each iteration. Inspired by the approach, the original ToG [ 48] paper introduces, we modify the algorithm replacing costly LLM calls with fast ad-hoc scoring and pruning heuristics. We use a smaller LLM to predict the 5 candidate edges to explore and match each one of the most similar existing edge at that step using S-BERT’s semantic encoding capabilities. We For each of the candidate tail nodes, we then comput a local connectivity score with the respect to the existing path and combine the two, selecting the best ones to extend the paths beam. Due to computational and time costs of running the algorithm, even though it is faster the original implementation, we still only had enough time to run experiments for a single configuration with the maximum depth of 4. The results were presented in the Figure 4.4 and Table 4.4. The average accuracy was 35%, which is 2% above the baseline, but is significantly below relevance scoring peers, 10% and 12% below BM25 and S-BERT respectively. ToG-LP managed to produce competitive results on FBQA but went below the baseline level for CWQ and MetaQA. The likely cause for that is bias towards Freebase and has to be explored further. Due to how the testing data was collected, it was not possible to study the performance of individual components of the algorithm in detail. Future work should address this by collecting and analysing more fine-grained data on the algorithm. As the 2 main components driving the exploration are both heuristics, it is worth pointing out the limitations of either, which significantly contribute to the final performance.Chapter 5. Discussion 36 First, a small LLM, while offering faster inference, has a reduced context window size becoming a bottleneck further along the graph, and much weaker ”emergent capabilities” [17], which are the bedrock of neural QA paradigm. Local connectivity score is perhaps the most limiting factor, while it is logically sound and quick to compute, it suffers the drawbacks of statistical approach, mainly data sparsity. Because the graphs are incomplete or sparsely connected, and paths are relatively short, the score would be biased towards hub nodes. Graph structure needs to be studied in more detail to design a more appropriate tail pruning heuristic. 5.4 Case Study: CWQ Question Types ComplexWebQuestions [ 38] constructs a dataset that requires complex reasoning along the Freebase [ 35] knowledge graph. The dataset contains 4 types of questions (as shown in Table 5.2). Question Type Example Comparative What country with an ISO less than 233 borders Russia? CompositionWhat actress plays Claire on a television show with the theme song ’Lonely Girl’ ? ConjunctionIn what country is Arabic spoken and is the birthplace of Barbara Starr ? SuperlativeFrom all the sights in Madrid, what is the latest exhibition venue that opened in that city? Table 5.1: Different types of questions and their examples as provided by the Com- plexWebQuestions dataset.Chapter 5. Discussion 37 Figure 5.1: Mistral7B performance on CWQ question types. Coloured lines present different heuristics, dashed line presents the current SOTA. Method Comparative Composition Conjunction Superlative Average Baseline 0.150 0.177 0.213 0.212 0.188 Perfect-Path 0.283 0.576 0.430 0.423 0.428 BM25- n 0.164 0.151 0.473 0.113 0.225 S-BERT- n 0.262 0.269 0.275 0.170 0.244 ToG-LP-4 0.248 0.111 0.157 0.132 0.162 Table 5.2: Average performance for each type of CWQ question. Rows represent different heuristics and their performance on each type of question, the final column is the average. include the table with averages and comment on what methods performs best for each question type. Comment on class balance when selecting a subset