{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.llm.mistral import MistralLLM\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = MistralLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/model'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"3_link_prediction/KGQA.pdf\"\n",
    "pdf = PdfReader(pdf_path)\n",
    "diss_text = \"\"\n",
    "for page in pdf.pages[39:43]:\n",
    "    diss_text += page.extract_text()\n",
    "    diss_text = diss_text.replace(\"\\n\", \" \")\n",
    "\n",
    "with open(\"3_link_prediction/KGQA.txt\", \"w\") as f:\n",
    "    f.write(diss_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In section 5.3, the results of different types of questions are discussed, focusing on the ComplexWebQuestions (CWQ) dataset. The CWQ dataset consists of four types of questions: Comparative, Composition, Conjunction, and Superlative.\\n\\n1. Comparative: What country with an ISO less than 233 borders Russia?\\n2. Composition: What actress plays Claire on a television show with the theme song ’Lonely Girl’?\\n3. Conjunction: In what country is Arabic spoken and is the birthplace of Barbara Starr?\\n4. Superlative: From all the sights in Madrid, what is the latest exhibition venue that opened in that city?\\n\\nThe results show that the performance of the models varies across different question types. Here are the average performances for each type of question and the overall average:\\n\\n| Method | Comparative | Composition | Conjunction | Superlative | Average |\\n|--------|-------------|-------------|-------------|-------------|---------|\\n| Baseline | 0.150       | 0.177        | 0.213        | 0.212        | 0.188    |\\n| Perfect-Path | 0.283       | 0.576        | 0.430        | 0.423        | 0.428    |\\n| BM25-n | 0.164       | 0.151        | 0.473        | 0.113        | 0.225    |\\n| S-BERT-n | 0.262       | 0.269        | 0.275        | 0.170        | 0.244    |\\n| ToG-LP-4 | 0.248       | 0.111        | 0.157        | 0.132        | 0.162    |\\n\\nThe Perfect-Path method performs the best on Comparative questions, while it excels on Composition questions. BM25-n performs relatively well on Composition questions, but its performance is poor on Comparative, Conjunction, and Superlative questions. S-BERT-n performs well on Comparative and Composition questions, but its performance is poor on Conjunction and Superlative questions. ToG-LP-4 performs relatively well on Comparative questions but underperforms on Composition, Conjunction, and Superlative questions.\\n\\nIt is worth noting that the class balance when selecting a subset of questions is not discussed in the provided text. However, it is essential to consider class balance when evaluating the performance of models on imbalanced datasets, as it can significantly impact the results.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = mistral(\"\"\"\n",
    "Summarise the results on different types of questions in section 5.3. \\n\\n\"\"\" + diss_text)\n",
    "display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 5.3, the results of different types of questions are discussed, focusing on the ComplexWebQuestions (CWQ) dataset. The CWQ dataset consists of four types of questions: Comparative, Composition, Conjunction, and Superlative.\\n\\n1. Comparative: What country with an ISO less than 233 borders Russia?\\n2. Composition: What actress plays Claire on a television show with the theme song ’Lonely Girl’?\\n3. Conjunction: In what country is Arabic spoken and is the birthplace of Barbara Starr?\\n4. Superlative: From all the sights in Madrid, what is the latest exhibition venue that opened in that city?\\n\\nThe results show that the performance of the models varies across different question types. Here are the average performances for each type of question and the overall average:\\n\\n| Method | Comparative | Composition | Conjunction | Superlative | Average |\\n|--------|-------------|-------------|-------------|-------------|---------|\\n| Baseline | 0.150       | 0.177        | 0.213        | 0.212        | 0.188    |\\n| Perfect-Path | 0.283       | 0.576        | 0.430        | 0.423        | 0.428    |\\n| BM25-n | 0.164       | 0.151        | 0.473        | 0.113        | 0.225    |\\n| S-BERT-n | 0.262       | 0.269        | 0.275        | 0.170        | 0.244    |\\n| ToG-LP-4 | 0.248       | 0.111        | 0.157        | 0.132        | 0.162    |\\n\\n\n",
    "\n",
    "The Perfect-Path method performs the best on Comparative questions, while it excels on Composition questions. BM25-n performs relatively well on Composition questions, but its performance is poor on Comparative, Conjunction, and Superlative questions. S-BERT-n performs well on Comparative and Composition questions, but its performance is poor on Conjunction and Superlative questions. ToG-LP-4 performs relatively well on Comparative questions but underperforms on Composition, Conjunction, and Superlative questions.\\n\\nIt is worth noting that the class balance when selecting a subset of questions is not discussed in the provided text. However, it is essential to consider class balance when evaluating the performance of models on imbalanced datasets, as it can significantly impact the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
